rm(list=ls())
library(tidyverse)

#################################
#### Load data and functions ####
#################################

# Note: this is not the exact data that was used for the study. The actual Truven MarketScan data is restricted.
# However, this is a simulated dataset that has the same underlying distribution and will behave similarly.
load("data/cftr_data.RData")

# load Functions for analysis
source("R/cftr_functions.R")

###################
#### View Data ####
###################

# These are indicators of the different conditions that were analyzed. The variable id, identifies the individual patient
dx_indicators

# These are the ids of the cases of patients with CF. Strata defines the age-sex-enrollment strata they belong to
# and strata-size defines the size of the given strata in the study population
cf_cases

# These are the matched non-CF cases
non_carriers %>% 
  unnest()

#################################
#### Define study parameters ####
#################################

# Case counts for the original study population and number of controls (note: controls were matched 5:1 with cases)
case_count <- 19802
control_count <- case_count*5

# expected number of mis-classified cases based on standard screening panels
expected_misses <- round(0.00295*case_count) 

######################################
#### Test the simulation function ####
######################################

# The draw_misclass_cohort() draws an intentionally misclassified cohort composed of (1) non-carriers labeled as 
# carrier cases, (2) miss-classified carriers that actually have CF labeled as cases and (3) non-carriers labeled 
# as controls. This function takes an integer input that defines the expected number of missclassified carriers 
# (i.e., the number of individuals labeled as carriers that we think might actually have CF). Note: the cases are 
# generate here will we made up of non-carriers (with no increased risk) and missclassified patients with CF (who 
# are known to be at increased risk.)
draw_misclass_cohort(expected_misses) 

# The function compute_odds_ratios() goes through each of the conditions and computes the odds ratio that carriers have
# the given condition relative to controls. If you look inside the details of this function, you will see that it is simply
# computing the odds ratio from the 4x4 matrix. However, in the actual study we used a conditional logistic regression, 
# this can be done using the clogit() function in the survival package...this takes longer to compute so we are using simple
# hand calculations here.
draw_misclass_cohort(expected_misses) %>% 
  compute_odds_ratios()

##########################################################################################
### SIMULATION 1 - Estimate the probability that results are due to misclassification ####
##########################################################################################


# Here is a quick example with only 10 trials. For each trial, we simply run the two commands described above. Here
# I am using the map function to loop over the column trial. Alternatively you could use a for loop or one of the
# apply functions in R.
sim_res <- tibble(trial=1:10) %>%
  mutate(res=map(trial,~draw_misclass_cohort(expected_misses) %>%
                   compute_odds_ratios()))

# To view the results we need to unnest the dataframe
sim_res %>% 
  unnest(res)

# I have pre-run 100 trials which are included in the file cftr_simulation_results.RData. If you uncomment the 
# following lines you can generate these results. (Note: this may take a few minutes to run)

# sim_res <- tibble(trial=1:100) %>%
#   mutate(res=map(trial,~draw_misclass_cohort(expected_misses) %>%
#                    compute_odds_ratios()))

# load in the pre-computed results
load("data/cftr_simulation_results.RData")

# Finally for each condition we can compute a type of p-value, corresponding to the hypothesis that the results 
# for any single condition were generated by misclassification. 
sim_res %>% 
  unnest() %>% 
  gather(key=name,value=sim_or,-trial) %>% 
  inner_join(original_results) %>% 
  group_by(name) %>% 
  summarise(p_val=sum(sim_or>est_or)/n())

#####################################################################################
#### SIMULATION 2 - Estimate the total number of conditions with similar results ####
#####################################################################################

# The second simulation we performed was across all conditions...to assess the 
# likelihood of obtaining similar results across all 59 different conditions. 
# Specifically, we tested the hypothesis that 59 conditions could return results 
# >= the observed study results simultaneously. Note: in the actual study we ran 
# millions of simulations. 

# For this simulation we can actually just use the results from above...we 
# simply have to count the number of significant 
sim_res %>% 
  unnest() %>% 
  gather(key=name,value=sim_or,-trial) %>% 
  inner_join(original_results) %>% 
  filter(sim_or>est_or) %>% 
  group_by(trial) %>% 
  summarise(num_sig_conditions=n()) %>% 
  count(num_sig_conditions) 

##################################################
#### SIMULATION 3 - Estimate an empirical FDR ####
##################################################

# For the third simulation we tested the hypothesis that the results we generated 
# could be due to "False Discovery" by performing  multiple statistical tests. 
# Because we ran 59 different statistical tests, it may be the case that some of 
# these results could have occurred by chance. To test this possibility we simply 
# repeatedly draw study cohorts, using the functions above, where we do not 
# include missclassified Carriers (i.e., both cases and controls are non-carriers).
# We then re-run our analysis and summarize the number of significant results we 
# return.

# This simulation takes a long-time to run, and to speed up the process, we use 
# parallel computing. The results of this simulation are also included in the 
# results file loaded above. To regenerate these results uncomment the following.

# library(multidplyr)
# cluster <- create_cluster(5) 
# cluster_library(cluster, "tidyverse")
# cluster_library(cluster, "survival")
# parallel::clusterExport(cluster, list("compute_odds_ratios","draw_misclass_cohort","get_paired_or",
#                                       "cf_cases","dx_indicators","non_carriers","original_results",
#                                       "case_count","control_count","expected_misses"))
# 
# 
# sim_res2 <- tibble(trial=1:100) %>% 
#   partition(trial, cluster = cluster) %>% 
#   mutate(res=map(trial,~draw_misclass_cohort(0) %>% 
#                    get_paired_or())) %>% 
#   collect() %>% 
#   ungroup() %>% 
#   arrange(trial)

#save(sim_res,sim_res2,file='simulation_results.RData')


# Using the dataset "sim_res2", that was loaded above, we can now compute an 
# empirical false discovery rate...note based on this overly simple analysis we 
# only obtain ~2-3 significant results. However, in the actual study we performed 
# an analysis that was slightly more complex...so the results are slightly different.
sim_res2 %>% 
  unnest() %>% 
  group_by(trial) %>% 
  summarise(n_sig=sum(p_val<0.05,na.rm=T),
            fdr=n_sig/n()) %>% 
  summarise(mean_fdr=mean(fdr),
            median_fdr=median(fdr),
            mean_n_sig=mean(n_sig),
            median_n_sig=median(n_sig))
